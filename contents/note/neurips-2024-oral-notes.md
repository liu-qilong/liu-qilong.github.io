---
title: NeurIPS 2024 oral notes
tags:
  - Hinton/ConferenceReview
date: "2025-02-28"
update: 
link:
  link: https://neurips.cc
---

# NeurIPS 2024 oral notes

## Domain

- Generative AI
	- [ChunlinTian2024NeurIPS](https://openreview.net/forum?id=qEpi8uWX3N) investigates the parameter inefficiency of LoRA and proposes an improvement
	- 3D generation
		- [MinghuaLiu2024NeurIPS](https://openreview.net/forum?id=x7pjdDod6Z) generates mesh with 3D sparse voxels as representation, instead of triplane #ToRead
		  *P.S. Trained with 8xH100 for 1 week*
	- Image generation
		- [KeyuTian2024NeurIPS](https://openreview.net/forum?id=gojL67CfS8) presents a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine "next-scale prediction", diverging from the standard raster-scan "next-token prediction"
		  It presents two important properties of LLMs: scaling laws and zero-shot task generalization #ToRead
		- [TianhongLi2024NeurIPS+](https://openreview.net/forum?id=clTa4JFBML) improves unconditioned image generation by using latent representation to conditioned the image generation process
	- Video generation
		- [SichengXu2024NeurIPS](https://openreview.net/forum?id=5zSCSE0k41) learns a disentangled face latent space for facial dynamics and head motion. It's then used for audio to facial video conversion in real-time
- LLM
	- [ZhenghaoLin2024NeurIPS](https://openreview.net/forum?id=0NMzBwqaAJ) scores training tokens using a reference model and then training the language model with a focused loss on tokens with higher scores selectively, proving "not all tokens are what you need"
	- Agent
		- [GabrielPoesia2024NeurIPS](https://openreview.net/forum?id=uNKlTQ8mBD) replicates the axiom-conjuncture-proving scheme of human mathematician
		- [ShangziXue2024NeurIPS](https://openreview.net/forum?id=NPKZF1WDjZ) introduces a reasoning tree framework consisting of decompose-analyze-rethink, noted that the decompose step builds sub-trees while the rethink step reflect & update the parent tree
		- [JingchangChen2024NeurIPS](https://openreview.net/forum?id=cFqAANINgW) replicates the idea of divide-and-conquer for code generation
		- [ShaotengLiu2024NeurIPS](https://openreview.net/forum?id=LEzx6QRkRH) break down a task into subtasks and dynamically decide whether to solve such a subtask by code generated by the LLM or by a "traditional" RL agent
	- Quantization
		- [HaokunLin2024NeurIPS](https://openreview.net/forum?id=mp8u2Pcmqz) utilizes rotation and permutation transformations to more effectively mitigate both massive and normal outliers when quantizing LLMs
		- [VladimirMalinovskii2024NeurIPS](https://openreview.net/forum?id=YvA8UF0I37) proposes a quantization-aware strategies for fine-tuning the LLMs after quantization, improving its performance especially in extreme-compression
	- Alignment
		- [JiamingJi2024NeurIPS](https://openreview.net/forum?id=kq166jACVP) proposes Aligner, a small model that learns the correctional residuals between preferred and dis-preferred answers, which can be used as a model-agnostic, plug-and-play alignment module with only one-off training
	- Evaluation
		- [ZheHu2024NeurIPS](https://openreview.net/forum?id=bCMpdaQCNW) evaluates vision language models' abilities to understand human humor with the YesBut dataset
		- [RicardoDominguezOlmedo2024NeurIPS](https://openreview.net/forum?id=Oo7dlLgqQX) evaluates LLM's answer to survey questions that's designed for human and reveals that it suffers strong positional bias; if the positional bias is properly controlled, it represents aggregated uniform. Therefore, it's dangerous to interpret its response to survey as if it's a real human being
		- [ArjunPanickssery2024NeurIPS](https://openreview.net/forum?id=4NJBV6Wp0h) evaluates the self-preference bias when LLM acting as an evaluator, which is an issue with wide implications in LLM benchmarking, reward modeling, and self-refinement
		- [QiguangChen2024NeurIPS](https://openreview.net/forum?id=pC44UMwy2v) introduces the reasoning boundary (RB) for (a) the quantitative metrics to assess CoT capabilities and (b) explains how certain strategies optimizes CoT performance
	- Multimodal
		- [ShengbangTong2024NeurIPS](https://openreview.net/forum?id=Vi8AepAXGy) explores the design choices of the vision components of MLLMs
		- [ChengyiCai2024NeurIPS](https://openreview.net/forum?id=135eKqDoRR) uses Bayesian-guided label mapping for visual reprogramming, in replacement of the simple one-to-one mapping of the training and downstream labels
- CV
	- 2D vision
		- [JiaqingZhang2024NeurIPS](https://openreview.net/forum?id=47loYmzxep) for single stage end-to-end training of multi-modal fusion detection
		- [zhengruiXu2024NeurIPS](https://openreview.net/forum?id=OycU0bAus6) uses diffusion model as feature extractor for discriminative tasks
		- [MichaelLuo2024NeurIPS](https://openreview.net/forum?id=3Odq2tGSpp) automatically selects and composes task-specific adapters for diffusion models based on a user-provided prompt
	- 3D vision
		- Reconstruction
			- [RuiqiGao2024NeurIPS](https://openreview.net/forum?id=TFZlFRl9Ks) uses multi-view diffusion model to generate novel views for 3D reconstruction
		- Segmentation
			- [ChangliWu2024NeurIPS](https://openreview.net/forum?id=r5spnrY6H3) uses spatial information to enhance 3D referring expression segmentation
		- Spatial-temporal
			- [JunhaoCai2024NeurIPS](https://openreview.net/forum?id=SSCtCq2MH2) uses Gaussians for simulation & physical property estimation
			- [ZhongchaoYi2024NeurIPS](https://openreview.net/forum?id=tnh4LK72yj) cooperative multi-dimensional and multi-task learning for urban intelligence
- Recommendation
	- [ShenLi2024NeurIPS](https://openreview.net/forum?id=aIPwlkdOut) proposes to use response time as a cue to learn human preference. Specifically, it combines choices and response times to estimate human utility functions, grounded in the EZ diffusion model from psychology.
	  _P.S. It claims combining such extra-info accelerates the preference learning process. Hum... Claiming extra-info boost performance sounds trivial, yet claiming that it boosts learning sounds brilliant! Clever one._
- AI4Science
	- Casual inference
		- [FengXie2024NeurIPS](https://openreview.net/forum?id=S2P6KPLtm8) theoretically investigates the identification of the bi-directional MR from observational data and develops a cluster fusion-like method for causal inference
		- [SiyuanGuo2024NeurIPS](https://openreview.net/forum?id=4rCZeCZAON) develops a casual inference framework for exchangeable generative processes, which naturally arise in multi-environment data, extending current works from i.i.d. (independent and identically distributed) to non i.i.d. settings
	- Math
		- [ZekunShi2024NeurIPS](https://openreview.net/forum?id=J2wI2rCG2u) uses stochastic Taylor derivative estimator for efficient amortization of differential operators, boosting the speed of high-order differential operators in large-scale problems, e.g. solving PDE and running Physics-Informed Neural Networks (PINNs)
	- Physics & chemistry
		- [GangLiu2024NeurIPS](https://openreview.net/forum?id=cfrDLD1wfO) proposes Graph DiT for conditioned molecular design, with a condition encoder to learn the representation of numerical and categorical properties and a Transformer-based graph denoiser to achieve molecular graph denoising
		- [NicholasGao2024NeurIPS](https://openreview.net/forum?id=HRkniCWM3E) designs the over-parametrized & fully learnable neural wave functions, facilitating the use of learnable generalized wave functions for simulating the ground state of many-electron systems
		- [YuliaRubanova2024NeurIPS](https://openreview.net/forum?id=QDYts5dYgq) uses learned signed-distance functions (SDFs) to represent the object shapes and to speed up distance computation for GNN based rigid simulation. It's the fist GNN-based simulator that scale to scenes with hundreds of objects and up to 1.1 million nodes
	- Neuroscience
		- [SpencerRooke2024NeurIPS](https://openreview.net/forum?id=REIK4SZMJt) finds that (i) the number of contexts storable by the hippocampus grows exponentially with the number of place cells; and (ii) identifies a trade-off between high resolution encoding of position and the number of storable contexts
		- [ZixuanGong2024NeurIPS](https://openreview.net/forum?id=8qu52Fl1Dt) proposes NeuroClips for fMRI-to-video decoding. It first reconstruct video keyframes from high-level semantics flow, and then injects both keyframes and low-level perception flows to a pre-trained T2V diffusion model for video reconstruction
	- Healthcare
		- [YubinKim2024NeurIPS](https://openreview.net/forum?id=EKdk4vxKO4) introduce a multi-agent framework that enforce a collaboration structure to a team of LLMs for medical decision

## Architecture

- Diffusion model
	- [AntonioTerpin2024NeurIPS](https://openreview.net/forum?id=y10avdRFNK) enhances the training speed
	- [TianweiYin2024NeurIPS](https://openreview.net/forum?id=tQukGCDaNT) enhances one-step generation by improve the distillation scheme with two time-scale update and GAN loss (rather than images sampled from the teacher model)
	- [TeroKarras2024NeurIPS](https://openreview.net/forum?id=bg6fVPVs3s) guides the model with a smaller, less-trained model, leading to improved variation, comparing with using an unconditional model
	- [SangwoongYoon2024NeurIPS](https://openreview.net/forum?id=V0oJaLqY4E) uses inverse reinforcement learning (IRL) to improve the sample quality of diffusion generative model
- Transformer
	- [TianyuHe2024NeurIPS](https://openreview.net/forum?id=aVh9KRZdRk) investigates transformer model's out-of-distribution in-context learning ability with a set of constructed arithmetic tasks
	- [YuhongChou2024NeurIPS](https://openreview.net/forum?id=Y8YVCOMEpz) unifies existing linear complexity attention and proposes Meta Linear Attention (MetaLA), to replace the conventional softmax attention
	- [YutaoSun2024NeurIPS](https://openreview.net/forum?id=25Ioxw576r) introduce a decoder-decoder architecture, YOCO, which only caches key-value pairs once, to reduce RAM demands and prefill latency
- GNN
	- [DongxiaoHe2024NeurIPS](https://openreview.net/forum?id=R8SolCx62K) reveals the common mechanism behind various contrastive representation learning for GNN as representation scattering and proposes the Scattering Graph Representation Learning (SGRL) framework
	- [RaffaelePaolino2024NeurIPS](https://openreview.net/forum?id=9O2sVnEHor) introduces a new hierarchy of graph isomorphism tests, alternative to the standard k-WL hierarchy
	- [IoannisKalogeropoulos2024NeurIPS](https://openreview.net/forum?id=8Fxqn1tZM1) proposes a new GNN-based meta-networks design to include _scaling symmetries_, instead of only _permutation symmetries_ that has been investigated before
- CNN
	- [FelixPetersen2024NeurIPS++](https://openreview.net/forum?id=4bKEFyUHT4) uses logic gate to replicate CNN
- Traditional machine learning
	- [ArthurdaCunha2024NeurIPS](https://openreview.net/forum?id=rtz4df9IF1) close the gap of the Boosting's theoretical lower bound of the $p$ (the number of training rounds)  - $t$ (the total parallel work per round) trade-off
	- [JinZhang2024NeurIPS](https://openreview.net/forum?id=m1a4CrRJR7) establishes generalization upper bound of Rademacher complexity for various tree-based retrievers
	- [XinChen2024NeurIPS](https://openreview.net/forum?id=ge8GZn8Gtu) achieves optimal clustering in Gaussian Mixture Models with anisotropic covariance structures

## Supervision

- Reinforcement learning
  _Due to the lack of proper knowledge, I barely understand anything of these papers..._
	- [XiongHuiChen2024NeurIPS](https://openreview.net/forum?id=Ddak3nSqQM) introduces policy learning from tutorial books and verifies the idea by outperforming GPT-agent without using real data for training
	- [JaydenTeoh2024NeurIPS](https://openreview.net/forum?id=UdxpjKO2F9) proposes coverage-based novelty evaluation for unsupervised environmental design (UED)
	- [DengweiZhao2024NeurIPS](https://openreview.net/forum?id=mSaqxZVZW8) enhances A* heuristic search with selective sampling
	- [PhilipAmortila2024NeurIPS](https://openreview.net/forum?id=qf2uZAdy1N) explores the latent dynamics
	- Theoretical analysis
		- [YangPeng2024NeurIPS](https://openreview.net/forum?id=eWUM5hRYgH) analyzes the finite-sample performance of distributional temporal difference leading (TD)
		- [MatthewZurek2024NeurIPS](https://openreview.net/forum?id=pGEY8JQ3qx) analyzes the optimal sample complexity
		- [SudeepSalgia2024NeurIPS](https://openreview.net/forum?id=6YIpvnkjUK) investigates the trade-off between sample and communication complexity in federated Q-learning
- Federated learning
	- [YongzheJia2024NeurIPS](https://openreview.net/forum?id=Pezt0xttae) introduces a FL framework to address system heterogeneity and domain shifts in edge computing environments. It employs a Model Fusion Pruning (MFP) module to generate personalized compact local models and a Domain Adaptive Regularization (DAR) module to enhance performance across multiple domains
- Knowledge distillation
	- [HaonanLin2024NeurIPS+](https://openreview.net/forum?id=C4NbtYnyQg) aligns teacher's & student's attention
- [AaronDefazio2024NeurIPS](https://openreview.net/forum?id=0XeNkkENuI) introduces schedule-free AdamW, without additional hyper-parameters over standard optimizers with momentum. It's based on the authors' theory that unifies scheduling and iterate averaging
- [RohanAlur2024NeurIPS](https://openreview.net/forum?id=wpGJ2AX6SZ) for human in the loop to incorporate side information that are algorithmically indistinguishable